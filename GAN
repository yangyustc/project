import tensorflow.compat.v1 as tf
import numpy as np
import matplotlib.pyplot as plt


tf.disable_eager_execution()


def img_inputs(real_size, noise_size):
    """
    real img and fak img

    """
    real_img = tf.placeholder(tf.float32, [None, real_size], name='real_img')
    noise_img = tf.placeholder(tf.float32, [None, noise_size], name='noise_img')

    return real_img, noise_img


def view_samples(epoch, samples):
    fig, axes = plt.subplots(figsize=(7, 7), nrows=5, ncols=5, sharey=True, sharex=True)
    for ax, img in zip(axes.flatten(), samples[epoch][1]):
        ax.xaxis.set_visible(False)
        ax.yaxis.set_visible(False)
        im = ax.imshow(img.reshape((28, 28)), cmap='Greys_r')
    plt.show()
    return fig, axes

def img_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01):
    """
    generator

    noise_img: imput img
    n_units: hidden layer nodes
    out_dim: size of output
    alpha: leaky ReLU
    """

    with tf.variable_scope("generator", reuse=reuse):
        # hidden layer

        hidden1 = tf.layers.dense(noise_img, n_units)
        # leaky ReLU
        hidden1 = tf.maximum(alpha * hidden1, hidden1)
        # dropout
        hidden1 = tf.layers.dropout(hidden1, rate=0.2)

        hidden1 = tf.layers.dense(hidden1, n_units)
        # leaky ReLU
        hidden1 = tf.maximum(alpha * hidden1, hidden1)
        # dropout
        hidden1 = tf.layers.dropout(hidden1, rate=0.2)

        # logits & outputs
        logits = tf.layers.dense(hidden1, out_dim)
        outputs = tf.tanh(logits)

        return logits, outputs


def img_discriminator(img, n_units, reuse=False, alpha=0.01):
    """
    discriminator

    n_units: hidden layer nodes
    alpha: Leaky ReLU
    """
    with tf.variable_scope("discriminator", reuse=reuse):
        # hidden layer
        hidden1 = tf.layers.dense(img, n_units)
        hidden1 = tf.maximum(alpha * hidden1, hidden1)
        hidden1 = tf.layers.dropout(hidden1, rate=0.2)

        hidden1 = tf.layers.dense(hidden1, n_units)
        hidden1 = tf.maximum(alpha * hidden1, hidden1)
        hidden1 = tf.layers.dropout(hidden1, rate=0.2)

        # logits & outputs
        logits = tf.layers.dense(hidden1, 1)
        outputs = tf.sigmoid(logits)

        return logits, outputs


(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

img_size = 784
noise_size = 100
# generator hidden layer nodes
g_units = 256
# discriminator hidden layer nodes
d_units =128
# leaky ReLU
alpha = 0.01
# learning_rate
learning_rate = 0.001
# label smoothing
smooth = 0.1

#set up

tf.reset_default_graph()

real_img, noise_img = img_inputs(img_size, noise_size)

# generator
g_logits, g_outputs = img_generator(noise_img, g_units, img_size)

# discriminator
d_logits_real, d_outputs_real = img_discriminator(real_img, d_units)
d_logits_fake, d_outputs_fake = img_discriminator(g_outputs, d_units, reuse=True)

# discriminator loss
d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,
                                                                     labels=tf.ones_like(d_logits_real)) * (1 - smooth))
# fake img
d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,
                                                                     labels=tf.zeros_like(d_logits_fake)))
# total loss
d_loss = tf.add(d_loss_real, d_loss_fake)

# generator loss
g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,
                                                                labels=tf.ones_like(d_logits_fake)) * (1 - smooth))

train_vars = tf.trainable_variables()

g_vars = [var for var in train_vars if var.name.startswith("generator")]
d_vars = [var for var in train_vars if var.name.startswith("discriminator")]

# optimizer
d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)
g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)

# batch_size
batch_size = 100
# epoch
epochs = 300
# sample num
n_sample = 25

# save sample
samples = []
# save loss
losses = []
# save generator
saver = tf.train.Saver(var_list=g_vars)
# train
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        for batch_i in range(600):
            start=batch_i*batch_size
            end=start+batch_size
            batch = train_images[start:end]

            batch_images = batch.reshape((batch_size, 784))
            batch_images = batch_images * 2.0/256- 1

            batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))

            # Run optimizers
            _ = sess.run(d_train_opt, feed_dict={real_img: batch_images, noise_img: batch_noise})
            _ = sess.run(g_train_opt, feed_dict={noise_img: batch_noise})

        train_loss_d = sess.run(d_loss,
                                feed_dict={real_img: batch_images,
                                           noise_img: batch_noise})
        # real img loss
        train_loss_d_real = sess.run(d_loss_real,
                                     feed_dict={real_img: batch_images,
                                                noise_img: batch_noise})
        # fake img loss
        train_loss_d_fake = sess.run(d_loss_fake,
                                     feed_dict={real_img: batch_images,
                                                noise_img: batch_noise})
        # generator loss
        train_loss_g = sess.run(g_loss,
                                feed_dict={noise_img: batch_noise})

        losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))

        sample_noise = np.random.uniform(-1, 1, size=(n_sample, noise_size))
        gen_samples = sess.run(img_generator(noise_img, g_units, img_size, reuse=True),
                               feed_dict={noise_img: sample_noise})
        samples.append(gen_samples)
        d_logits_fake, d_outputs_fake = sess.run(img_discriminator(gen_samples[1], d_units, reuse=True))

        print("Epoch {}/{}...".format(e + 1, epochs),
              "Discriminator Loss: {:.4f}(Real: {:.4f} + Fake: {:.4f})...".format(train_loss_d, train_loss_d_real,
                                                                                  train_loss_d_fake),
              "Generator Loss: {:.4f}".format(train_loss_g))
        print(e)

        saver.save(sess, './checkpoints/GAN_2h256n.ckpt')

_ = view_samples(-1, samples)
np.savetxt('./checkpoints/GANloss_2h256n.txt',losses)
